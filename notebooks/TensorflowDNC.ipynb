{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralComputer.ipynb   Untitled1.ipynb        tasks_1-20_v1-2.tar.gz\r\n",
      "TensorflowDNC.ipynb    Untitled2.ipynb\r\n",
      "Untitled.ipynb         \u001b[1m\u001b[36mbAbi\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./summary && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input will be a sequence of 26 words, padding by zeros at the beginning when needed.\n",
      "There are 25 unique words, which will be mapped to one-hot encoded vectors.\n",
      "There are 3 unique answers, which will be mapped to one-hot encoded vectors.\n",
      "\n",
      "Encoding sequences...\n",
      "Splitting training/testing set...\n",
      "\n",
      "X: (1000, 26, 25)\n",
      "y: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if not os.path.exists(\"bAbi\"):\n",
    "    !wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
    "    !mkdir bAbi && tar -xzf ./tasks_1-20_v1-2.tar.gz -C bAbi --strip-components 1\n",
    "    \n",
    "unique_words = set([''])\n",
    "unique_answers = set()\n",
    "for file in glob.glob(\"bAbi/en-10k/*\"):\n",
    "    stories = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = [str(re.sub(\"\\d\", \"\", line)).strip() for line in f.readlines()]\n",
    "        story = []\n",
    "        for line in lines:\n",
    "            line = line.replace(\",\", \" , \").replace(\"?\", \" ? \")\n",
    "            line = line.replace(\".\", \" . \").replace(\",\", \" , \").replace('\\'','')\n",
    "            \n",
    "            if \"\\t\" not in line:\n",
    "                # not a question\n",
    "                words = line.split()\n",
    "                unique_words = unique_words.union(set(words))\n",
    "                story.extend(words)\n",
    "            else:\n",
    "                # question\n",
    "                [line, answer] = line.split(\"\\t\")\n",
    "                words = line.split()\n",
    "                unique_words = unique_words.union(set(words))\n",
    "                unique_answers = unique_answers.union(set([answer]))\n",
    "                story.extend(words)\n",
    "                this_story = {\n",
    "                        \"seq\": story,\n",
    "                        \"answer\": answer\n",
    "                }\n",
    "                stories.append(this_story)\n",
    "                story = []\n",
    "    break\n",
    "\n",
    "longest_story = max(stories, key=lambda s: len(s[\"seq\"]))\n",
    "longest_story_len = len(longest_story[\"seq\"])\n",
    "print(\"Input will be a sequence of {} words, \"\\\n",
    "      \"padding by zeros at the beginning when \"\\\n",
    "      \"needed.\".format(len(longest_story[\"seq\"])))\n",
    "\n",
    "num_words = len(unique_words)\n",
    "num_answers = len(unique_answers)\n",
    "print(\"There are {} unique words, which will be mapped to one-hot encoded vectors.\".format(num_words))\n",
    "print(\"There are {} unique answers, which will be mapped to one-hot encoded vectors.\".format(num_answers))\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "word_encoder = lb.fit(list(unique_words))\n",
    "\n",
    "lba = preprocessing.LabelBinarizer()\n",
    "answer_encoder = lba.fit(list(unique_answers))\n",
    "\n",
    "def pad_and_encode_seq(seq, seq_len=longest_story_len):\n",
    "    if len(seq) > seq_len:\n",
    "        raise RuntimeError(\"Should never see a sequence greater than {} length\".format(seq_len))\n",
    "    return word_encoder.transform((['' for i in range(seq_len-len(seq))]) + seq)\n",
    "\n",
    "print()\n",
    "print(\"Encoding sequences...\")\n",
    "X = []\n",
    "y = []\n",
    "from collections import defaultdict\n",
    "\n",
    "for story in stories:\n",
    "    X.append(np.array(pad_and_encode_seq(story[\"seq\"])))\n",
    "    y.append(answer_encoder.transform([story[\"answer\"]])[0])\n",
    "   \n",
    "print(\"Splitting training/testing set...\")\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print()\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "def one_plus(x):\n",
    "    return 1 + tf.log(1 + tf.exp(x))\n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = tf.exp(x - tf.reduce_max(x, reduction_indices=[0]))\n",
    "    return e_x / tf.reduce_sum(e_x)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    # Formula for cosine similarity\n",
    "    r = tf.cast(tf.reduce_sum(tf.mul(a, b)) / (tf.sqrt(tf.reduce_sum(tf.square(a))) * tf.sqrt(tf.reduce_sum(tf.square(b)))), tf.float64)    \n",
    "    # We don't really want NaN if the denominator is 0, let's just set to 0 in that case.\n",
    "    r = tf.select(tf.is_nan(r), tf.constant(0.0, dtype=tf.float64), r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of [ 1.  0.  0.] and [ 1.  1.  1.] is 0.577.\n",
      "Sigmoid of [ 1000. -1000.] is [ 1.  0.]\n",
      "One_plus of [ 1000. -1000.] is [ inf   1.]\n",
      "Softmax of [ 2.  1.] is [ 0.7310586   0.26894143]\n"
     ]
    }
   ],
   "source": [
    "## Cosine Similarity ##\n",
    "U = tf.constant([1.0, 0.0, 0.0])\n",
    "V = tf.constant([1.0, 1.0, 1.0])\n",
    "sim = cosine_similarity(U, V)\n",
    "\n",
    "v1 = tf.constant([1000.0, -1000.0])\n",
    "v2 = tf.constant([2.0, 1.0])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Cosine similarity of {} and {} is {:0.3f}.\".format(U.eval(), V.eval(), sim.eval()))\n",
    "    print(\"Sigmoid of {} is {}\".format(v1.eval(), sigmoid(v1).eval()))\n",
    "    print(\"One_plus of {} is {}\".format(v1.eval(), one_plus(v1).eval()))\n",
    "    print(\"Softmax of {} is {}\".format(v2.eval(), softmax(v2).eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content_lookup(M, k, B):\n",
    "    \"\"\"Content lookup for a single read/write head.\n",
    "    \n",
    "    Args:\n",
    "        M (tf.Tensor): Memory matrix with dimensions (N, W).\n",
    "        k (tf.Tensor): Read/write key emitted by the controller\n",
    "            with dimensions (W,).\n",
    "        B/beta (tf.Tensor): Read/write strength emitted by the \n",
    "            controller which is an int. This represents how \n",
    "            strongly you want your head to attend to the closest \n",
    "            matching memory location (B=1 shows almost no \n",
    "            preference, B=100 will almost always single out\n",
    "            the closest matching location).\n",
    "            \n",
    "    Returns:\n",
    "        np.array: normalized probability distribution over \n",
    "            the locations in memory with dimensions (N,).\n",
    "            You can think of this as the attention that the\n",
    "            read head pays to each location based on the \n",
    "            content similarity.\n",
    "    \n",
    "    \"\"\"\n",
    "    #locations = tf.map_fn(lambda x: np.exp(cos_sim(k, x.reshape(1, -1)) * B),1, M)\n",
    "    locations = tf.map_fn(lambda x: tf.exp(cosine_similarity(x, k)*B), M)\n",
    "    r = locations / tf.reduce_sum(locations)\n",
    "    \n",
    "    # Again, if we get a NaN here, we would like that to represent a 1.\n",
    "    r = tf.select(tf.is_nan(r), tf.ones(r._shape, dtype=tf.float64), r)\n",
    "    return tf.squeeze(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Memory ==\n",
      "\n",
      "[[ 1.   0.   0. ]\n",
      " [ 1.   1.   1. ]\n",
      " [ 0.8  0.4  0.2]]\n",
      "\n",
      "== Key ==\n",
      "\n",
      "Key:  [ 1.  1.  1.]\n",
      "Beta: 20.0\n",
      "\n",
      "== Content ==\n",
      "\n",
      "[  1.94851797e-04   9.13678345e-01   8.61268030e-02]\n"
     ]
    }
   ],
   "source": [
    "M = tf.constant([[1.0, 0.0, 0.0],\n",
    "                [1.0, 1.0, 1.0],\n",
    "                [0.8, 0.4, 0.2]], dtype=tf.float64)\n",
    "k = tf.constant([1.0, 1.0, 1.0], dtype=tf.float64)\n",
    "beta = tf.constant(20.0, dtype=tf.float64)\n",
    "\n",
    "content = content_lookup(M, k, beta)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"== Memory ==\\n\\n{}\\n\".format(M.eval()))\n",
    "    print(\"== Key ==\\n\\nKey:  {}\\nBeta: {}\\n\".format(k.eval(), beta.eval()))\n",
    "    print(\"== Content ==\\n\\n{}\".format(content.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tf_argsort(v):\n",
    "    return tf.py_func(np.argsort, [v], [tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.scalar_summary('stddev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batches(X, y, batch_size=1):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    length = len(indices)\n",
    "    np.random.shuffle(indices)\n",
    "        \n",
    "    for ndx in range(0, length, batch_size):\n",
    "        curr_idxs = indices[ndx:min(ndx + batch_size, length)]\n",
    "        yield (np.array(X[curr_idxs]), np.array(y[curr_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have enough defined to create our differentiable neural computer class, which will handle all of the coupling between our controller and the memory network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "tf.float32 = tf.float64\n",
    "\n",
    "class DNC(object):\n",
    "    \"\"\"Differential Neural Computer implementation\n",
    "    in tensorflow. You can find this paper here: http://go.nature.com/2dIULo5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, validation_split=0.25, N=256, W=64, R=2, n_hidden=512, batch_size=1,\n",
    "                disable_memory=False, use_linkage=True):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_split)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.N = N\n",
    "        self.W = W\n",
    "        self.R = R\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.disable_memory=disable_memory\n",
    "        self.use_linkage = use_linkage\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                \n",
    "        # Controller settings #\n",
    "        (self.n_train_instances, self.n_timesteps, self.n_env_inputs) = self.X_train.shape\n",
    "        (self.n_test_instances, _, _) = self.X_test.shape\n",
    "        (_, self.n_classes) = self.y_train.shape\n",
    "        self.n_read_inputs = self.W*self.R\n",
    "        self.n_interface_outputs = (self.W*self.R) + 3*self.W + 5*self.R + 3\n",
    "        \n",
    "        # Tensorflow settings #\n",
    "        self.session = tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "                   intra_op_parallelism_threads=1))\n",
    "        self.reset_memory_state()\n",
    "        self.compile() \n",
    "        \n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        \n",
    "    def reset_memory_state(self):\n",
    "        \n",
    "        # Just for convenience\n",
    "        N = self.N\n",
    "        W = self.W\n",
    "        R = self.R\n",
    "        \n",
    "        self.memory = tf.zeros((N, W), dtype=tf.float64)\n",
    "        \n",
    "        #########################\n",
    "        ## Read head variables ##\n",
    "        #########################\n",
    "        \n",
    "        self.read_keys = tf.zeros((R, W), dtype=tf.float64)\n",
    "        self.read_strengths = tf.zeros((R,), dtype=tf.float64)\n",
    "        self.free_gates = tf.zeros((R,), dtype=tf.float64)\n",
    "        self.read_modes = tf.zeros((R, 3), dtype=tf.float64)\n",
    "        self.read_weightings = tf.zeros((R, N), dtype=tf.float64)\n",
    "        \n",
    "        ##########################\n",
    "        ## Write head variables ##\n",
    "        ##########################\n",
    "        \n",
    "        self.write_key = tf.zeros((W,), dtype=tf.float64)\n",
    "        self.write_strength = tf.Variable(0.0, dtype=tf.float64)\n",
    "        self.write_gate = tf.Variable(0.0, dtype=tf.float64)\n",
    "        self.write_weighting = tf.zeros((N,), dtype=tf.float64)\n",
    "        \n",
    "        #####################\n",
    "        ## Other variables ##\n",
    "        #####################\n",
    "        \n",
    "        self.usage_vector = tf.zeros((N,), dtype=tf.float64)\n",
    "        self.write_vector = tf.zeros((W,), dtype=tf.float64)\n",
    "        self.erase_vector = tf.zeros((W,), dtype=tf.float64)\n",
    "        self.allocation_gate = tf.Variable(0.0, dtype=tf.float64)\n",
    "        self.precendence_weighting = tf.zeros((N,), dtype=tf.float64)\n",
    "        self.linkage_matrix = tf.zeros((N, N), dtype=tf.float64)\n",
    "        self.precedense = tf.zeros((N,), dtype=tf.float64)\n",
    "        \n",
    "    def compile(self):\n",
    "        \n",
    "        self.X_in = tf.placeholder(tf.float64, [self.batch_size, self.n_timesteps, self.n_env_inputs])\n",
    "        self.y_in = tf.placeholder(tf.float64, [self.batch_size, self.n_classes])\n",
    "        \n",
    "        weights = tf.Variable(tf.random_normal([self.n_hidden, self.n_classes], dtype=tf.float64))\n",
    "        biases = tf.Variable(tf.random_normal([self.n_classes], dtype=tf.float64))\n",
    "    \n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(self.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        state = lstm_cell.zero_state(self.batch_size, tf.float64)\n",
    "        \n",
    "        self.outputs = []\n",
    "        self.states = []\n",
    "        self.interfaces = []\n",
    "        self.read_keys = []\n",
    "        reads_in = tf.zeros((self.batch_size, self.n_read_inputs), dtype=tf.float64)\n",
    "        for i in range(self.n_timesteps):\n",
    "            with tf.variable_scope(\"LSTM{}\".format(i)):\n",
    "                input_ = tf.concat(1, [self.X_in[:,i,:], tf.expand_dims(tf.reshape(reads_in, [-1]), 0)])\n",
    "                output, state = lstm_cell(input_, state)\n",
    "                \n",
    "                self.outputs.append(output)\n",
    "                self.states.append(state)\n",
    "            \n",
    "            if i < self.n_timesteps-1:\n",
    "                with tf.variable_scope(\"Interface{}\".format(i)):\n",
    "                    iw = tf.Variable(tf.random_normal([self.n_hidden, self.n_interface_outputs], dtype=tf.float64), name='weights')\n",
    "                    ib = tf.Variable(tf.random_normal([self.n_interface_outputs], dtype=tf.float64), name='biases')\n",
    "                    interface = tf.matmul(output, iw) + ib\n",
    "                    with tf.variable_scope(\"reads\"):\n",
    "                        reads_in = self.get_reads(interface)\n",
    "                    self.interfaces.append(interface)\n",
    "                    \n",
    "        self.pred_fn = tf.matmul(output, weights) + biases\n",
    "        self.cost_fn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.pred_fn, self.y_in))\n",
    "        \n",
    "        # Create an optimizer.\n",
    "        self.opt_fn = tf.contrib.layers.optimize_loss(self.cost_fn,\n",
    "                                self.global_step,\n",
    "                                0.0001,\n",
    "                                'Adagrad',\n",
    "                                clip_gradients=10000.0,\n",
    "                                summaries=tf.contrib.layers.OPTIMIZER_SUMMARIES)\n",
    "                \n",
    "        # Evaluate model\n",
    "        self.correct_pred_fn = tf.equal(tf.argmax(self.pred_fn, 1), tf.argmax(self.y_in, 1))\n",
    "        self.accuracy_fn = tf.reduce_mean(tf.cast(self.correct_pred_fn, tf.float64))\n",
    "        #variable_summaries(self.cost_fn, \"loss\")\n",
    "        self.summaries = tf.merge_all_summaries()\n",
    "        self.summary_writer = tf.train.SummaryWriter(\"summary/\", self.session.graph)\n",
    "        \n",
    "    def assess(self):\n",
    "        accs = []\n",
    "        losses = []\n",
    "        \n",
    "        for (batch_x, batch_y) in make_batches(self.X_train, self.y_train, batch_size=self.batch_size):\n",
    "            accs.append(self.session.run(self.accuracy_fn, feed_dict={self.X_in: batch_x, self.y_in: batch_y}))\n",
    "            losses.append(self.session.run(self.cost_fn, feed_dict={self.X_in: batch_x, self.y_in: batch_y}))\n",
    "            \n",
    "        return np.array(accs, dtype=\"float32\").mean(), np.array(losses, dtype=\"float32\").mean()\n",
    "            \n",
    "    def train(self, n_iterations=1):\n",
    "        current_iter=0\n",
    "        \n",
    "        while current_iter < n_iterations:\n",
    "            i = 0\n",
    "            for (batch_x, batch_y) in make_batches(self.X_train, self.y_train, batch_size=self.batch_size):\n",
    "                _, loss, summaries = self.session.run([self.opt_fn, self.cost_fn, self.summaries], feed_dict={self.X_in: batch_x, self.y_in: batch_y})\n",
    "                self.summary_writer.add_summary(summaries, i)\n",
    "                self.summary_writer.flush()\n",
    "                percent = (i/self.n_train_instances)\n",
    "                progress_bar = \"[\"+'='*np.floor(percent*40)+'>'+' '*(40-np.floor(percent*40))+\"]\"\n",
    "                print(\"\\rIteration {}: {} {}/{} ({:.2f}%)\".format(current_iter+1, progress_bar, \n",
    "                    i, self.n_train_instances, percent*100.0), end=\"\")\n",
    "                i += self.batch_size\n",
    "\n",
    "            acc, loss = self.assess()\n",
    "            print(\"\\rIteration {}: Test Loss={:.6f}, Test Accuracy={:.5f}\".format(current_iter+1, loss, acc))\n",
    "            current_iter += 1\n",
    "            \n",
    "    def parse_interface(self, i_in):\n",
    "        offset = 0 \n",
    "        \n",
    "        # For convenience\n",
    "        R = self.R\n",
    "        W = self.W\n",
    "        N = self.N\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        read_keys = tf.reshape(tf.slice(i_in, [0, offset], [-1, R*W+offset]), (R, W))\n",
    "        offset += R*W\n",
    "        \n",
    "        read_strengths = tf.reshape(tf.slice(one_plus(i_in), [0, offset], [-1, R]), (R,))\n",
    "        offset += R\n",
    "        \n",
    "        write_key = tf.reshape(tf.slice(i_in, [0, offset], [-1, W]), (W,))\n",
    "        offset += W\n",
    "        \n",
    "        write_strength = one_plus(i_in[:, offset:offset+1])\n",
    "        offset += 1\n",
    "        \n",
    "        erase_vector = tf.reshape(tf.slice(sigmoid(i_in), [0, offset], [-1, W]), (W,))\n",
    "        offset += W\n",
    "        \n",
    "        write_vector = tf.reshape(tf.slice(i_in, [0, offset], [-1, W]), (W,))\n",
    "        offset += W\n",
    "        \n",
    "        free_gates = tf.reshape(tf.slice(sigmoid(i_in), [0, offset], [-1, R]), (R,))\n",
    "        offset += R\n",
    "        \n",
    "        allocation_gate = sigmoid(i_in[:, offset:offset+1])\n",
    "        offset += 1\n",
    "        \n",
    "        write_gate = sigmoid(i_in[:, offset:offset+1])\n",
    "        offset += 1\n",
    "        \n",
    "        read_modes = tf.reshape(tf.slice(softmax(i_in), [0, offset], [-1, R*3]), (R, 3))\n",
    "        offset += R*3\n",
    "        \n",
    "        return read_keys, read_strengths, write_key, write_strength, \\\n",
    "               erase_vector, write_vector, free_gates, allocation_gate, \\\n",
    "               write_gate, read_modes\n",
    "        \n",
    "    def get_reads(self, interface_out):\n",
    "        if self.disable_memory:\n",
    "            return tf.zeros((self.batch_size, self.n_read_inputs), dtype=tf.float64)\n",
    "        \n",
    "        ##################\n",
    "        # Parse reads in #\n",
    "        ##################\n",
    "        \n",
    "        read_keys, read_strengths, write_key, write_strength, \\\n",
    "        erase_vector, write_vector, free_gates, allocation_gate, \\\n",
    "        write_gate, read_modes = self.parse_interface(interface_out)\n",
    "        \n",
    "        ###################\n",
    "        # Content lookups #\n",
    "        ###################\n",
    "        \n",
    "        read_content_lookups = tf.pack([content_lookup(self.memory, K, b) for (K, b) in zip(tf.unpack(read_keys), \n",
    "                                                                                            tf.unpack(read_strengths))])\n",
    "        \n",
    "        write_content_lookup = content_lookup(self.memory, write_key, write_strength)\n",
    "        \n",
    "        ##########################\n",
    "        # Update read weightings #\n",
    "        ##########################\n",
    "        \n",
    "        results = []\n",
    "        for (r_w, c_t, pi) in zip(tf.unpack(self.read_weightings), \n",
    "                                  tf.unpack(read_content_lookups), \n",
    "                                  tf.unpack(read_modes)):\n",
    "            if self.use_linkage:\n",
    "                f_t = tf.reduce_sum(self.linkage_matrix * r_w, reduction_indices=[0])\n",
    "                b_t = tf.reduce_sum(tf.transpose(self.linkage_matrix) * r_w, reduction_indices=[0])\n",
    "                results.append(pi[0]*b_t + pi[1]*c_t + pi[2]*f_t)\n",
    "            else:\n",
    "                results.append(pi[1]*c_t)\n",
    "                \n",
    "        self.read_weightings = tf.pack(results)\n",
    "        \n",
    "        ##########################\n",
    "        # Update write weighting #\n",
    "        ##########################\n",
    "        \n",
    "        # (1) get retention vector\n",
    "        \n",
    "        retention_vector = tf.reduce_prod(tf.pack([1-f*wr for (f, wr) in zip(tf.unpack(self.free_gates), \n",
    "                                                                              tf.unpack(self.read_weightings))]),\n",
    "                                                                              reduction_indices=[0])\n",
    "\n",
    "        # (2) update usage vector\n",
    "        self.usage_vector = tf.mul(self.usage_vector + self.write_weighting \n",
    "                                   - tf.mul(self.usage_vector, self.write_weighting), retention_vector)\n",
    "        \n",
    "        # (3) get allocation weightings\n",
    "        phi = tf_argsort(self.usage_vector)[0]\n",
    "        allocation_vector_list = []\n",
    "        with tf.variable_scope(\"allocation_weightings\"):\n",
    "            for j in range(self.N):\n",
    "                part1 = (1 - tf.slice(self.usage_vector, [phi[j]], [1]))\n",
    "                part2 = tf.reduce_prod(tf.cast(tf.pack([tf.slice(self.usage_vector, [phi[i]], [1]) for i in range(j-1)]), tf.float64), reduction_indices=[0])\n",
    "                val = part1 + part2\n",
    "                allocation_vector_list.append(val)\n",
    "            \n",
    "            allocation_vector = tf.squeeze(tf.pack(allocation_vector_list))\n",
    "\n",
    "        # (4) update write weightings\n",
    "        wg = self.write_gate\n",
    "        ag = self.allocation_gate\n",
    "        aw = allocation_vector\n",
    "        c = write_content_lookup\n",
    "        #self.write_weighting = wg*(ag*aw + (1.0 - ag)*c)\n",
    "        self.write_weighting = wg*(c)\n",
    "        \n",
    "        ##################\n",
    "        # Linkage update #\n",
    "        ##################\n",
    "        \n",
    "        if self.use_linkage:\n",
    "            new_linkage_matrix = []\n",
    "        \n",
    "            # Update linkages\n",
    "            for i in range(self.N):\n",
    "                this_row = []\n",
    "                for j in range(self.N):\n",
    "                    if i == j:\n",
    "                        this_row.append(0.0) # diagonals == 0\n",
    "                    else:\n",
    "                        this_row.append((1 - self.write_weighting[i] - self.write_weighting[j])*self.linkage_matrix[i][j] + self.write_weighting[i]*self.precedense[j])\n",
    "                    \n",
    "                new_linkage_matrix.append(this_row)\n",
    "            \n",
    "            self.linkage_matrix = tf.squeeze(tf.pack(new_linkage_matrix))\n",
    "        \n",
    "        #################\n",
    "        # Update memory #\n",
    "        #################\n",
    "        \n",
    "        w_t = tf.expand_dims(self.write_weighting, 1)\n",
    "        et_T = tf.expand_dims(self.erase_vector, 0)\n",
    "        vt_T = tf.expand_dims(self.write_vector, 0)\n",
    "        self.memory = tf.mul(self.memory, tf.ones_like(self.memory) -  tf.matmul(w_t, et_T)) + tf.matmul(w_t, vt_T)\n",
    "\n",
    "        ################\n",
    "        # Read vectors #\n",
    "        ################\n",
    "        \n",
    "        results = []\n",
    "        for read_weighting in tf.unpack(self.read_weightings):\n",
    "            results.append(tf.transpose(self.memory) * read_weighting)\n",
    "        read_vectors = tf.squeeze(tf.pack(results))\n",
    "    \n",
    "        return read_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This takes quite a long time to compile... go grab a beer...finished!\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: OptimizeLoss/HistogramSummary_12\n\t [[Node: OptimizeLoss/HistogramSummary_12 = HistogramSummary[T=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OptimizeLoss/HistogramSummary_12/tag, OptimizeLoss/clip_by_global_norm/OptimizeLoss/clip_by_global_norm/_8)]]\nCaused by op 'OptimizeLoss/HistogramSummary_12', defined at:\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-357e6465a3f6>\", line 2, in <module>\n    dnc = DNC(X, y, W=10, N=10, R=1)\n  File \"<ipython-input-11-af25f2d89807>\", line 39, in __init__\n    self.compile()\n  File \"<ipython-input-11-af25f2d89807>\", line 125, in compile\n    summaries=tf.contrib.layers.OPTIMIZER_SUMMARIES)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 201, in optimize_loss\n    grad_values)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/logging_ops.py\", line 125, in histogram_summary\n    tag=tag, values=values, name=scope)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 100, in _histogram_summary\n    name=name)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: OptimizeLoss/HistogramSummary_12\n\t [[Node: OptimizeLoss/HistogramSummary_12 = HistogramSummary[T=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OptimizeLoss/HistogramSummary_12/tag, OptimizeLoss/clip_by_global_norm/OptimizeLoss/clip_by_global_norm/_8)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-357e6465a3f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDNC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-af25f2d89807>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_iterations)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: OptimizeLoss/HistogramSummary_12\n\t [[Node: OptimizeLoss/HistogramSummary_12 = HistogramSummary[T=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OptimizeLoss/HistogramSummary_12/tag, OptimizeLoss/clip_by_global_norm/OptimizeLoss/clip_by_global_norm/_8)]]\nCaused by op 'OptimizeLoss/HistogramSummary_12', defined at:\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-357e6465a3f6>\", line 2, in <module>\n    dnc = DNC(X, y, W=10, N=10, R=1)\n  File \"<ipython-input-11-af25f2d89807>\", line 39, in __init__\n    self.compile()\n  File \"<ipython-input-11-af25f2d89807>\", line 125, in compile\n    summaries=tf.contrib.layers.OPTIMIZER_SUMMARIES)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 201, in optimize_loss\n    grad_values)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/logging_ops.py\", line 125, in histogram_summary\n    tag=tag, values=values, name=scope)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 100, in _histogram_summary\n    name=name)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/claymcleod/miniconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "print(\"This takes quite a long time to compile... go grab a beer...\", end=\"\")\n",
    "dnc = DNC(X, y, W=10, N=10, R=1)\n",
    "print(\"finished!\")\n",
    "dnc.train(n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
